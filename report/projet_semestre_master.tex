\documentclass[10pt]{report}

\usepackage[utf8]{inputenc}

\usepackage{makeidx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}

%\usepackage{biblatex}
\title{Tilted Beta Extremal Mixtures}
\author{Samuel Sekarski \\ Supervisor: Prof. Davison}
\date{June 2020}

%\makeindex
%\addbibresource{bib.bib}

\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle


%\chapter*{Acknowledgements}

\tableofcontents
\listoffigures

\include{introduction}

\include{multivariate_extremes}

\include{statistical_aspects}

\chapter{Conclusions}
This fits to the real data are not bad, but could be a lot better. There are a few problems that should be looked at more in depth, such as using a constrained optimizer to solve the maximum likelihood estimation, to prevent the parameters from exploding. When one of the parameters explodes, but not the other one, which was often the case in our simulations, then the optimizer is fitting a distribution that tends to resemble a Dirac delta "distribution" in either 0 or 1, which can cause unforeseeable behaviour in the optimization algorithm as the computer is ill equipped to deal with infinite-like values. If both parameters explode, the fitted distribution tends to resemble a Dirac delta "distribution" in 0.5, and the same sort of problems could occur. The question is, how to chose the appropriate bounds, that stop this behaviour, while still allowing the correct parameters to be found, if by chance they are very large.
Another question, is if the optimizers are exploring the whole parameter space, or if they are getting stuck in local minimas, which might be the case when they fit only 1 component very well, then using more might actually yield a better result. 
This least also to another thing work asking, which is how to chose the initial value for the optimization algorithms. Currently we are just using 10 tuples of uniformly generated initial values within a bounded hypercube, and using the one that yield the best log likelihood. But there might be better ways to go about this.

We see also, that more of the time, a good enough fit is obtained by using just 1 or 2 components, and this is confirmed visually as well as by using a forward step-AIC method for selecting the number of parameters. There was just the case for location 1, where maybe a fit with $K=6$ would be the optimal, but numerically there were some problems with this model, so that I couldn't compute a KS test on it. But the model with $K=3$ is pretty good too.




\begin{thebibliography}{9}

\bibitem{ColesTawn}
S. Coles and J. A. Tawn, \textit{Modelling Extreme Multivariate Events}, Journal of the Royal Statistical Society, 1991, pp. 381-382

\bibitem{Coles}
S. Coles, \textit{An Introduction to Statistical Modelling of Extreme Values}, Springer Series in Statistics, 2001, p. 144


\bibitem{BoldiDavison}
M.-O. Boldi and A. C. Davison, \textit{A mixture model for multivariate extremes}, Journal of the Royal Statistical Society Series B,  2007, pp.217-218

\end{thebibliography}


\chapter*{Code}
All the \texttt{R} code, graphs, literature, used in this project and the \LaTeX\ for this very document, can be found at \url{https://github.com/Sekarski/MasterSemestreProject}.


%\printindex

\end{document}
